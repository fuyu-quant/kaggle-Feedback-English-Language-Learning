setting:
  text : "sample"
  seed : 3655
  column : 'cohesion'
  #setting : conventions'
  num_classes : 1
  device : 'cuda:0'
  fold : 0
  num_folds : 4
  use_tqdm : True
  data_path : "/content/drive/MyDrive/Colab Notebooks/kaggle/Feedback_Prize_3/make_dataset/"
  model_save_path : '/content/drive/MyDrive/Colab Notebooks/kaggle/Feedback_Prize_3/model/'
  num_epochs : 3
  max_length : 512
  train_batch_size : 1
  valid_batch_size : 2

wandb:
  project : 'Feedback Prize - English Language Learning'
  tags : 'sample'


model:
  model_name: 'microsoft/deberta-large'
  gradient_checkpointing: False
  apex : True
  gradient_accumulations_steps : 1
  use_awp : False
  start_awp_epoch : 1
  # 1epoch目の何割学習をしてからvalidationを始めるか(0~1)の間で設定
  valid_start : 0.2
  # 何ミニバッチに1回，validationをするか
  valid_frequency : 10

optimizer:
  optimizer_name: None
  # 全てのパラメータを最適化するか
  all_optimize : True
  # 8bit-optimizerの設定
  bnb_8bit : True
  weight_decay : 0.05
  lr : 1e-5
  eps : None
  betas : None

scheduler:
  scheduler_name : None
  # torchのスケジューラーの設定
  T_max : 10
  min_lr : 10
  T_0 : 10
  start_factor : 1
  end_factor: 0.1
  total_iters : 1
  # transformersのスケジューラーの設定
  gamma : 0.1
  num_warmup_steps : 100
  num_train_steps : 100