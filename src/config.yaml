setting:
  text : "sample"
  seed : 3655
  debug : True
  column : 'cohesion'
  #setting : conventions'
  num_classes : 1
  device : 'cuda:0'
  fold : 0
  num_folds : 4
  use_tqdm : True
  data_path : "/content/drive/MyDrive/Colab Notebooks/kaggle/Feedback_Prize_3/make_dataset/"
  model_save_path : '/content/drive/MyDrive/Colab Notebooks/kaggle/Feedback_Prize_3/model/'
  num_epochs : 2
  max_length : 512
  train_batch_size : 1
  valid_batch_size : 1


wandb:
  project : 'Feedback Prize - English Language Learning'
  tags : 'sample'


model:
  model_name: 'microsoft/deberta-large'
  gradient_checkpointing: True
  apex : True
  gradient_accumulations_steps : 1
  # apexをTrueにしないとawpは動かない設定になってる
  use_awp : True
  start_awp : 0.6
  # 1epoch目の何割学習をしてからvalidationを始めるか(0~1)の間で設定
  start_valid : 0.6
  # 何ミニバッチに1回，validationをするか
  valid_frequency : 10


optimizer:
  optimizer_name: None
  # 全てのパラメータを最適化するか
  all_optimize : True
  # 8bit-optimizerの設定
  bnb_8bit : True
  weight_decay : 0.05
  lr : 1e-6
  eps : None
  betas : None


scheduler:
  scheduler_name : None
  # torchのスケジューラーの設定
  T_max : 10
  min_lr : 10
  T_0 : 10
  start_factor : 1
  end_factor: 0.1
  total_iters : 1
  # transformersのスケジューラーの設定
  gamma : 0.1
  num_warmup_steps : 100
  num_train_steps : 100


awp:
  adv_lr : 1
  adv_eps : 0.2
  adv_param : 'weight'
  adv_step : 1