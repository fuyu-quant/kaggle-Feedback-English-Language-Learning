{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "authorship_tag": "ABX9TyNyv6ebxs6ofspbn2amcPb9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fuyu-quant/kaggle-Feedback-English-Language-Learning/blob/main/Feedback3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# „Çπ„ÇØ„É™„Éó„Éà„ÅÆÂÆüË°å\n"
      ],
      "metadata": {
        "id": "38HM37gPXqxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"GPU„ÅÆÂà©Áî®ÊôÇÈñì\")\n",
        "!cat /proc/uptime | awk '{print $1 /60 /60 \"hours\"}'\n",
        "print(\"Âà©Áî®‰∏≠„ÅÆGPU\")\n",
        "import torch\n",
        "torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "lm55_W0SorXj",
        "outputId": "5a942b5a-f918-468f-ab95-5ac9bfbebbf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU„ÅÆÂà©Áî®ÊôÇÈñì\n",
            "0.174408hours\n",
            "Âà©Áî®‰∏≠„ÅÆGPU\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla V100-SXM2-16GB'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## google drive"
      ],
      "metadata": {
        "id": "RaEY1sb4oavf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qpwmDUJuoa6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## clone"
      ],
      "metadata": {
        "id": "RukMFOehbZlQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEgeLr6gWCsm"
      },
      "outputs": [],
      "source": [
        "ACCESS_TOKEN = \"\"\n",
        "REPOSITORY = \"fuyu-quant/kaggle-Feedback-English-Language-Learning\"  # clone/push „Åó„Åü„ÅÑ„É™„Éù„Ç∏„Éà„É™\n",
        "USER = \"fuyu-quant\"\n",
        "MAIL = \"ulti4929@gmail.com\"\n",
        "\n",
        "WORKDIR = \"GitHub\"\n",
        "BRANCH = \"develop\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "%rm -r GitHub\n",
        "\n",
        "! git clone https://{USER}:{ACCESS_TOKEN}@github.com/{REPOSITORY}.git {WORKDIR}\n",
        "%cd {WORKDIR}\n",
        "! git config --global user.name {USER}\n",
        "! git config --global user.email {MAIL}\n",
        "! git remote set-url origin https:/{USER}:{ACCESS_TOKEN}@github.com/{REPOSITORY}.git\n",
        "! git checkout -b {BRANCH}\n",
        "! git branch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOcB71ucZEEW",
        "outputId": "dccdccce-0f6f-4022-d19a-02d62de3826c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'GitHub'...\n",
            "remote: Enumerating objects: 159, done.\u001b[K\n",
            "remote: Counting objects: 100% (159/159), done.\u001b[K\n",
            "remote: Compressing objects: 100% (107/107), done.\u001b[K\n",
            "remote: Total 159 (delta 81), reused 101 (delta 39), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (159/159), 37.65 KiB | 12.55 MiB/s, done.\n",
            "Resolving deltas: 100% (81/81), done.\n",
            "/content/GitHub\n",
            "Switched to a new branch 'develop'\n",
            "* \u001b[32mdevelop\u001b[m\n",
            "  main\u001b[m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ÂÆüË°å"
      ],
      "metadata": {
        "id": "amgrWoFdei6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "djer_-FAD8iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb„ÅÆÊúÄÂàù„ÅÆÂÖ•Âäõ„ÅØ„Äå2„Äç\n",
        "!python /content/GitHub/src/train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkqoHJ9kEPhs",
        "outputId": "2544950e-ce6a-4b37-fc8e-2bd74c8cb65d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================WARNING: DEPRECATED!==============================\n",
            "WARNING! This version of bitsandbytes is deprecated. Please switch to `pip install bitsandbytes` and the new repo: https://github.com/TimDettmers/bitsandbytes\n",
            "==============================WARNING: DEPRECATED!==============================\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtoma_tanaka\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/GitHub/src/wandb/run-20220919_073659-11rtsjlm\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmicrosoft/deberta-large-fold0-cohesion-sample\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/toma_tanaka/Feedback3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/toma_tanaka/Feedback3/runs/11rtsjlm\u001b[0m\n",
            "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight']\n",
            "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "  0% 0/2933 [00:00<?, ?it/s]You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "Batch loss: 0.2051 - Avg loss: 0.9546:   4% 109/2933 [00:41<16:53,  2.79it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f5c870a1dd0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1455, in _shutdown_workers\n",
            "    self._worker_result_queue.put((None, None))\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 87, in put\n",
            "    self._start_thread()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 165, in _start_thread\n",
            "    name='QueueFeederThread'\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 796, in __init__\n",
            "    self._daemonic = current_thread().daemon\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 1128, in daemon\n",
            "    assert self._initialized, \"Thread.__init__() not called\"\n",
            "KeyboardInterrupt: \n",
            "Batch loss: 0.2051 - Avg loss: 0.9546:   4% 109/2933 [00:41<17:56,  2.62it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GitHub/src/train.py\", line 507, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/hydra/main.py\", line 95, in decorated_main\n",
            "    config_name=config_name,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/hydra/_internal/utils.py\", line 396, in _run_hydra\n",
            "    overrides=overrides,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/hydra/_internal/utils.py\", line 453, in _run_app\n",
            "    lambda: hydra.run(\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/hydra/_internal/utils.py\", line 213, in run_and_report\n",
            "    return func()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/hydra/_internal/utils.py\", line 456, in <lambda>\n",
            "    overrides=overrides,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/hydra/_internal/hydra.py\", line 127, in run\n",
            "    configure_logging=with_log_configuration,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/hydra/core/utils.py\", line 186, in run_job\n",
            "    ret.return_value = task_function(task_cfg)\n",
            "  File \"/content/GitHub/src/train.py\", line 501, in main\n",
            "    training_loop(cfg, fold)\n",
            "  File \"/content/GitHub/src/train.py\", line 482, in training_loop\n",
            "    train_fn(cfg, model, train_dataloader, optimizer, epoch, scheduler, valid_dataloader, fold, best_score = np.inf)\n",
            "  File \"/content/GitHub/src/train.py\", line 279, in train_fn\n",
            "    batch_loss = model(input_ids, attention_mask, target)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/GitHub/src/train.py\", line 146, in forward\n",
            "    output_hidden_states = False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/deberta/modeling_deberta.py\", line 1004, in forward\n",
            "    return_dict=return_dict,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/deberta/modeling_deberta.py\", line 498, in forward\n",
            "    output_attentions=output_attentions,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/deberta/modeling_deberta.py\", line 407, in forward\n",
            "    intermediate_output = self.intermediate(attention_output)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/deberta/modeling_deberta.py\", line 362, in forward\n",
            "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1205, in __getattr__\n",
            "    if name in modules:\n",
            "KeyboardInterrupt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 104.05247\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmicrosoft/deberta-large-fold0-cohesion-sample\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/toma_tanaka/Feedback3/runs/11rtsjlm\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220919_073659-11rtsjlm/logs\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}